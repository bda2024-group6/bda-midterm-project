{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ckiptagger/model_ws.py:106: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n",
      "2024-04-21 22:44:00.893601: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ckiptagger/model_pos.py:56: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ckiptagger/model_ner.py:57: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n"
     ]
    }
   ],
   "source": [
    "# Import packages and functions for part 1\n",
    "import pandas as pd\n",
    "from ckiptagger import WS, POS, NER\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from scipy import sparse\n",
    "\n",
    "# 先把我們需要的函數載入\n",
    "ws = WS(\"./data_ckip\") # 斷詞\n",
    "pos = POS(\"./data_ckip\") # 詞性標注\n",
    "ner = NER(\"./data_ckip\") # 命名實體識別"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用中研院 [CKIP Tagger](https://github.com/ckiplab/ckiptagger) 進行斷詞、詞性標記，篩選出文章中的**普通名詞**、**動詞**與**形容詞**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(sentence_ws, sentence_pos):\n",
    "    '''\n",
    "    - sentence_ws: 經過斷詞的句子\n",
    "    - sentence_pos: 經過詞性標注的句子\n",
    "    - 留下特定的詞性（名詞、動詞、形容詞）、排除一個字的詞、專有名詞\n",
    "    '''\n",
    "    cleaned = []\n",
    "    for word, pos in zip(sentence_ws, sentence_pos):\n",
    "        is_Na_or_V_or_A_or_D = pos.startswith(\"Na\") or pos.startswith(\"V\") or pos.startswith(\"A\")\n",
    "        if is_Na_or_V_or_A_or_D and len(word) > 1:\n",
    "            cleaned.append(word)\n",
    "    return \" \".join(cleaned)\n",
    "\n",
    "# Word segmentation\n",
    "def word_segmentation(contents, ws_driver, pos_driver, ner_driver):\n",
    "    '''\n",
    "    - contents: 一個 list，每個元素是一篇文章的全文內容\n",
    "    - ws: 斷詞模型\n",
    "    '''\n",
    "    ws_results = ws_driver(contents)\n",
    "    pos_results = pos_driver(ws_results)\n",
    "    contents_cleaned = []\n",
    "    for sentence, sentence_ws, sentence_pos in zip(contents, ws_results, pos_results):\n",
    "        sentence_cleaned = clean(sentence_ws, sentence_pos)\n",
    "        contents_cleaned.append(sentence_cleaned)\n",
    "\n",
    "    return contents_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_news = pd.read_csv('../data/news_filtered_merged.csv')\n",
    "contents = data_news['content'].tolist()\n",
    "contents_cleaned = word_segmentation(contents, ws, pos, ner)\n",
    "# add the cleaned content back to the dataframe as a column named 'content_cleaned'\n",
    "data_news['content_cleaned'] = contents_cleaned\n",
    "data_news.to_csv('../data/news_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate cleaned data and save\n",
    "def generate_cleaned_data(name, data, ws, pos, ner):\n",
    "    contents = data['content'].tolist()\n",
    "    contents_cleaned = word_segmentation(contents, ws, pos, ner)\n",
    "    data['content_cleaned'] = contents_cleaned\n",
    "    data.to_csv(f'../data/{name}_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate cleaned data for ptt and dcard\n",
    "data_ptt = pd.read_csv('../data/ptt_filtered_labeled.csv')\n",
    "data_dcard = pd.read_csv('../data/dcard_filtered_labeled.csv')\n",
    "data_to_clean = {'ptt': data_ptt, 'dcard': data_dcard}\n",
    "for name, data in data_to_clean.items():\n",
    "    generate_cleaned_data(name, data, ws, pos, ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from scipy import sparse\n",
    "\n",
    "# parameters \n",
    "n_features = {'ptt': 500, 'dcard': 500, 'news': 500, 'all': 500}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用 `TfidfVectorizer` 建構詞彙的向量空間。\n",
    "  - 考慮 1-gram 至 3-gram 的詞彙組合。\n",
    "  - 篩選掉 document frequency 大於文件數 95% 與出現小於 2 次的詞彙。\n",
    "- 對上述向量空間，我們使用 chi-square 選出前 k 名的最佳特徵作為模型的輸入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(source, data, n_features, days_ahead, chip = False): # chip 是籌碼數據\n",
    "    '''\n",
    "    - source: the source of the data\n",
    "    - data: a pandas dataframe with a column named 'content_cleaned'\n",
    "    - n_features: the number of features to select\n",
    "    - days_ahead: the number of days ahead to predict\n",
    "    '''\n",
    "    # TF-IDF\n",
    "    vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, ngram_range=(1, 3))\n",
    "    X = vectorizer.fit_transform(data['content_cleaned'])\n",
    "    # select the top 1000 features\n",
    "    ch2 = SelectKBest(chi2, k=min(n_features[source],X.shape[1] - 30)) \n",
    "    X = ch2.fit_transform(X, data['label_day'+str(days_ahead)])\n",
    "    if chip:\n",
    "        # add the chip data\n",
    "        # add the column of 'foreign_investor_surplus', 'investment_trust_surplus', 'dealer_surplus' to X\n",
    "        chip_data = data[['foreign_investor_surplus', 'investment_trust_surplus', 'dealer_surplus']].values\n",
    "        X = sparse.hstack((X, chip_data))\n",
    "    # get the target variable\n",
    "    y = data['label_day'+str(days_ahead)]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Model Training for All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from scipy.stats import randint\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "# initialize all parameters\n",
    "params = {\n",
    "    'Naive Bayes': {},\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1],\n",
    "        #'kernel': ['linear', 'rbf', 'poly'],\n",
    "        'kernel': ['rbf'],\n",
    "        'gamma': ['scale'],\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "採用模型：Naive Bayes, SVM, Random Forest, XGBoost 與 stacking model\n",
    "\n",
    "- 將資料隨機切分成訓練資料集 (80%) 與測試資料集 (20%)，並對訓練資料集進行 5-fold cross validation\n",
    "- 用 Grid Search 找到 Naive Bayes, SVM, Random Forest, XGBoost 的最佳參數\n",
    "- 再分別執行最佳模型與四者的 stacking model，紀錄結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(source, data, n_features, days_ahead, backtest=False, starting_year_month=None, training_months=None, chip = False):\n",
    "    '''\n",
    "    - source: the source of the data\n",
    "    - data: a pandas dataframe with a column named 'content_cleaned'\n",
    "    - n_features: the number of features to select\n",
    "    - days_ahead: the number of days ahead to predict\n",
    "    '''\n",
    "    # Drop the rows where label = -1\n",
    "    data = data[data['label_day'+str(days_ahead)] != -1]\n",
    "    result = {}\n",
    "    # feature extraction\n",
    "    X, y = feature_extraction(source, data, n_features, days_ahead, chip = chip)\n",
    "    # densify the sparse matrix\n",
    "    X = X.toarray()\n",
    "    # split the data into training and testing sets\n",
    "    if backtest:\n",
    "        print(\"Training on the data from\", starting_year_month, \"to\", starting_year_month + pd.DateOffset(months=training_months))\n",
    "        train_range = (data['date'] >= starting_year_month) & (data['date'] < starting_year_month + pd.DateOffset(months=training_months))\n",
    "        print(\"Testing on the data from\", starting_year_month + pd.DateOffset(months=training_months), \"to\", starting_year_month + pd.DateOffset(months=training_months+1))\n",
    "        test_range = (data['date'] >= starting_year_month + pd.DateOffset(months=training_months)) & (data['date'] < starting_year_month + pd.DateOffset(months=training_months+1))\n",
    "        X_train, X_test = X[train_range], X[test_range]\n",
    "        y_train, y_test = y[train_range], y[test_range]\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print('Training set:', X_train.shape, y_train.shape)\n",
    "    print('Testing set:', X_test.shape, y_test.shape)\n",
    "    # kfold\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    models = {\n",
    "        'Naive Bayes': GaussianNB(),\n",
    "        'Random Forest': RandomForestClassifier(),\n",
    "        'SVM': SVC(),\n",
    "        'XGBoost': XGBClassifier()\n",
    "    }\n",
    "    # search for the optimal parameters for each model by grid search\n",
    "    best_models = {}\n",
    "    for model_name, model in models.items():\n",
    "        grid_search = GridSearchCV(model, params[model_name], cv=kfold, scoring='accuracy', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_models[model_name] = grid_search.best_estimator_\n",
    "    # stack the models\n",
    "    stack = StackingClassifier(estimators=[(name, model) for name, model in best_models.items()], final_estimator=LogisticRegression())\n",
    "    stack.fit(X_train, y_train)\n",
    "    # predict the test set and calculate the accuracy and confusion matrix, and save the results\n",
    "    # best_models (respectively)\n",
    "    for name, model in best_models.items():\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        confusion = confusion_matrix(y_test, y_pred)\n",
    "        result[name] = {'accuracy': accuracy, 'confusion': confusion, 'prediction': y_pred}\n",
    "    # stack\n",
    "    y_pred = stack.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "    result['stack'] = {'accuracy': accuracy, 'confusion': confusion, 'prediction': y_pred}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 任務一：針對不同內容種類 (ptt, dcard, 新聞) 選取不同的 d (天數)和 k (特徵數) 進行模型訓練，並比較結果。\n",
    "- 把所有資料 (ptt, dcard, 新聞) 串接起來，選取 d = 3 和不同的 k (特徵數) 進行模型訓練，並比較結果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "datas = {\n",
    "    'all': pd.read_csv('../data/all_cleaned.csv'),\n",
    "    'ptt': pd.read_csv('../data/ptt_cleaned.csv'),\n",
    "    'dcard': pd.read_csv('../data/dcard_cleaned.csv'),\n",
    "    'news': pd.read_csv('../data/news_cleaned.csv'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models for all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (7374, 500) (7374,)\n",
      "Testing set: (1844, 500) (1844,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [06:13<24:52, 373.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (7428, 500) (7428,)\n",
      "Testing set: (1858, 500) (1858,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [08:17<33:08, 497.25s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m results[name] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m days_ahead \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6\u001b[39m)):\n\u001b[0;32m----> 9\u001b[0m     results[name][days_ahead] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdays_ahead\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 40\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(source, data, n_features, days_ahead, backtest, starting_year_month, training_months, chip)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     39\u001b[0m     grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(model, params[model_name], cv\u001b[38;5;241m=\u001b[39mkfold, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m     \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     best_models[model_name] \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# stack the models\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the models for each source media and get the results\n",
    "results = {}\n",
    "for name, data in datas.items():\n",
    "    # drop nan for content_cleaned\n",
    "    data = data.dropna(subset=['content_cleaned'])\n",
    "    print(f'Training models for {name}')\n",
    "    results[name] = {}\n",
    "    for days_ahead in tqdm.tqdm(range(1, 6)):\n",
    "        results[name][days_ahead] = train(name, data, n_features, days_ahead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with data['all'] for days ahead = 3 and number of features\n",
    "n_features_all = [10, 50, 100, 500, 800, 1000, 2000, 4000]\n",
    "results_all = {}\n",
    "datas['all'] = datas['all'].dropna(subset=['content_cleaned'])\n",
    "for num in n_features_all:\n",
    "    n_features['all'] = num\n",
    "    print(f'Training models for all with {num} features')\n",
    "    results_all[num] = train('all', datas['all'], n_features, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe to store the results\n",
    "def create_results_df(results):\n",
    "    results_df = pd.DataFrame(columns=['number of features', 'days_ahead', 'model', 'accuracy', 'confusion', 'prediction'])\n",
    "    for num, result in results.items():\n",
    "        for model, res in result.items():\n",
    "            df_to_concat = pd.DataFrame({'number of features': num, 'days_ahead': 3, 'model': model, 'accuracy': res['accuracy'], 'confusion': str(res['confusion']), 'prediction': str(res['prediction'])}, index=[0])\n",
    "            results_df = pd.concat([results_df, df_to_concat], ignore_index=True)\n",
    "    return results_df\n",
    "\n",
    "results_df = create_results_df(results_all)\n",
    "results_df.sort_values(by='accuracy', ascending=False, inplace=True)\n",
    "results_df.to_csv('../data/results/results_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the results\n",
    "results_flat = {}\n",
    "for name, result in results.items():\n",
    "    for days_ahead, models in result.items():\n",
    "        for model_name, model_result in models.items():\n",
    "            result_name = f'{name}_{days_ahead}_{model_name}'\n",
    "            results_flat[result_name] = model_result\n",
    "\n",
    "# save the flattened results\n",
    "results_df = pd.DataFrame(results_flat).T\n",
    "results_df.sort_values(by='accuracy', ascending=False, inplace=True)\n",
    "results_df\n",
    "results_df.to_csv(f'../data/prob2_sorted_f{n_features['news']}.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 移動回測\n",
    "對每一組四個月的資料，我們將前三個月視為訓練資料集、第四個月視為測試資料集，選取 d = 3 和不同的 k (特徵數) 進行模型訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a list from 2022-03 to 2023-12\n",
    "months = pd.date_range(start='2022-03-01', end='2023-12-01', freq='M')\n",
    "months = months.strftime('%Y-%m').tolist()\n",
    "\n",
    "# num_features\n",
    "n_features['all'] = 500\n",
    "\n",
    "# load the cleaned data\n",
    "datas = {\n",
    "    'all': pd.read_csv('../data/all_cleaned.csv'),\n",
    "}\n",
    "\n",
    "# test each month by train the data in the previous three months\n",
    "results = {}\n",
    "for name, data in datas.items():\n",
    "    print(f'Training models for {name}')\n",
    "    data = data.dropna(subset=['content_cleaned'])\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    results[name] = {}\n",
    "    for month in tqdm(months):\n",
    "        starting_month = pd.to_datetime(month)\n",
    "        training_months = 3  # timedelta of 3 month\n",
    "        results[name][month] = train(name, data, n_features, 3, backtest=True, starting_year_month=starting_month, training_months=training_months)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "對於每一個月，選擇表現最佳的模型並儲存其預測結果與模型準確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the results and save\n",
    "results_flat = {}\n",
    "df_backtest_result = pd.DataFrame(columns=['starting month', 'model', 'accuracy', 'confusion', 'prediction'])\n",
    "for name, result in results.items():\n",
    "    for month, models in result.items():\n",
    "        for model_name, model_result in models.items():\n",
    "            row = {'starting month': month, 'model': model_name, 'accuracy': model_result['accuracy'], 'confusion': str(model_result['confusion']), 'prediction': str(model_result['prediction'])}\n",
    "            df_backtest_result = pd.concat([df_backtest_result, pd.DataFrame(row, index=[0])], ignore_index=True)\n",
    "\n",
    "\n",
    "# select the best model for each month\n",
    "df_backtest_result['accuracy'] = df_backtest_result['accuracy'].astype(float)\n",
    "best_models = df_backtest_result.groupby('starting month').apply(lambda x: x.loc[x['accuracy'].idxmax()]).reset_index(drop=True)\n",
    "best_models.to_csv(f'../data/results/backtest_best_models_f{n_features[\"all\"]}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5. Make prediction and compute the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prediction data\n",
    "prediction_datas = {\n",
    "    500: pd.read_csv('../data/results/backtest_best_models_f500.csv'),\n",
    "    800: pd.read_csv('../data/results/backtest_best_models_f800.csv'),\n",
    "    1000: pd.read_csv('../data/results/backtest_best_models_f1000.csv'),\n",
    "    '500_chip': pd.read_csv('../data/results/backtest_best_models_f500_chip.csv'),\n",
    "}\n",
    "\n",
    "days_ahead = 3\n",
    "\n",
    "# prepare the data\n",
    "data_all = pd.read_csv('../data/all_cleaned.csv')\n",
    "data_all = data_all.dropna(subset=['content_cleaned'])\n",
    "data_all['date'] = pd.to_datetime(data_all['date'])\n",
    "data_all = data_all[data_all['label_day'+str(days_ahead)] != -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 對每天的文章用上一部分的最佳模型進行預測，並將預測結果輸出成 csv 檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a list from 2022-03 to 2023-12\n",
    "months = pd.date_range(start='2022-03-01', end='2023-12-01', freq='M')\n",
    "months = months.strftime('%Y-%m').tolist()\n",
    "\n",
    "training_months = 3  # timedelta of 3 month\n",
    "\n",
    "df_prediction_results = {num: pd.DataFrame()  for num in prediction_datas.keys()}\n",
    "\n",
    "for num, data in prediction_datas.items():\n",
    "    # print(f'Number of features: {num}')\n",
    "    for month in months:\n",
    "        predictions_str = data[data['starting month'] == month]['prediction'].iloc[0]\n",
    "        predictions = predictions_str[1:-1].replace('\\n', '').replace('.', '').split(' ')\n",
    "        predictions = np.array(predictions).astype(int)\n",
    "        month_to_predict = pd.to_datetime(month) + pd.DateOffset(months=training_months)\n",
    "        data_to_predict = data_all[(data_all['date'] >= month_to_predict) & (data_all['date'] < month_to_predict + pd.DateOffset(months=1))]\n",
    "        # compare the length of the predictions and the data to predict\n",
    "        # print(f'Month: {month}, Number of predictions: {len(predictions)}, Number of data to predict: {len(data_to_predict)}') \n",
    "        # add the prediction as a new column to the data\n",
    "        data_to_predict['prediction']= predictions\n",
    "        # compute the number of positive and negative predictions for each day\n",
    "        prediction_results = data_to_predict.groupby('date')['prediction'].value_counts().unstack().fillna(0)\n",
    "        # add the results to the dataframe\n",
    "        prediction_results = prediction_results.reset_index()\n",
    "        df_prediction_results[num] = pd.concat([df_prediction_results[num], prediction_results], axis=0)\n",
    "    # fill nan with 0\n",
    "    df_prediction_results[num].fillna(0, inplace=True)\n",
    "    # drop index\n",
    "    df_prediction_results[num].reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 拿預測結果實際和真實結果比較，計算出最後的出手率、準確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with real label\n",
    "df_label = data_all[['date', 'label_day'+str(days_ahead)]]\n",
    "df_label['date'] = pd.to_datetime(df_label['date'])\n",
    "# keep unique dates\n",
    "df_label = df_label.drop_duplicates(subset=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the label with the prediction results\n",
    "for num, df in df_prediction_results.items():\n",
    "    df = df.merge(df_label, on='date', how='left')\n",
    "    # 0 > 1 -> final predict 0; 1 > 0 -> final predict 1; tie -> final predict -1\n",
    "    df['final_prediction'] = np.where(df[0] > df[1], 0, np.where(df[1] > df[0], 1, -1))\n",
    "    df['make_decision'] = np.where(df[0] > df[1], 1, np.where(df[1] > df[0], 1, 0))\n",
    "    df['correct'] = np.where(df['final_prediction'] == df['label_day'+str(days_ahead)], 1, 0)\n",
    "    df['month'] = df['date'].dt.to_period('M')\n",
    "    df.to_csv(f'../data/results/predictions_by_date_f{num}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算每個月的出手率和準確率\n",
    "prediction_results = {\n",
    "    500: pd.read_csv('../data/results/predictions_by_date_f500.csv'),\n",
    "    800: pd.read_csv('../data/results/predictions_by_date_f800.csv'),\n",
    "    1000: pd.read_csv('../data/results/predictions_by_date_f1000.csv'),\n",
    "    '500_chip': pd.read_csv('../data/results/predictions_by_date_f500_chip.csv'),\n",
    "}\n",
    "\n",
    "df_final_rate = pd.DataFrame()\n",
    "for num, df in prediction_results.items():\n",
    "    # group by month\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df_grouped = df.groupby('month').agg({'correct': 'mean', 'make_decision':'mean'}).reset_index()\n",
    "    # drop index\n",
    "    df_grouped.reset_index(drop=True, inplace=True)\n",
    "    df_grouped.to_csv(f'../data/results/final_rate_f{num}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6. 加入籌碼數據進行實驗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將籌碼數據加入特徵中，重新進行第二部分的模型訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all_with_chip = pd.read_csv('../data/all_cleaned_chip.csv')\n",
    "results_with_chip = {}\n",
    "n_features['all'] = 1000\n",
    "data_all_with_chip = data_all_with_chip.dropna(subset=['content_cleaned'])\n",
    "results_all = train('all', data_all_with_chip, n_features, 3, chip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把上面的結果轉成 dataframe，並存成 csv 檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns=['number of features', 'days_ahead', 'model', 'accuracy', 'confusion', 'prediction'])\n",
    "for model_name, model_result in results_all.items():\n",
    "    new_row = {'number of features': 1000, 'days_ahead': 3, 'model': model_name, 'accuracy': model_result['accuracy'], 'confusion': str(model_result['confusion']), 'prediction': str(model_result['prediction'])}\n",
    "    results_df = pd.concat([results_df, pd.DataFrame(new_row, index=[0])], ignore_index=True)\n",
    "\n",
    "results_df.sort_values(by='accuracy', ascending=False, inplace=True)\n",
    "results_df.to_csv('../data/results/results_all_with_chip.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將籌碼數據加入特徵中，重新進行第三部分的移動回測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a list from 2022-03 to 2023-12\n",
    "months = pd.date_range(start='2022-03-01', end='2023-12-01', freq='M')\n",
    "months = months.strftime('%Y-%m').tolist()\n",
    "\n",
    "# num_features\n",
    "n_features['all_chip'] = 500\n",
    "\n",
    "# load the cleaned data\n",
    "datas = {\n",
    "    'all_chip': pd.read_csv('../data/all_cleaned_chip.csv'),\n",
    "}\n",
    "\n",
    "# test each month by train the data in the previous three months\n",
    "results = {}\n",
    "for name, data in datas.items():\n",
    "    print(f'Training models for {name}')\n",
    "    data = data.dropna(subset=['content_cleaned'])\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    results[name] = {}\n",
    "    for month in tqdm(months):\n",
    "        starting_month = pd.to_datetime(month)\n",
    "        training_months = 3  # timedelta of 3 month\n",
    "        results[name][month] = train(name, data, n_features, 3, backtest=True, starting_year_month=starting_month, training_months=training_months, chip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the results and save\n",
    "results_flat = {}\n",
    "df_backtest_result = pd.DataFrame(columns=['starting month', 'model', 'accuracy', 'confusion', 'prediction'])\n",
    "for name, result in results.items():\n",
    "    for month, models in result.items():\n",
    "        for model_name, model_result in models.items():\n",
    "            row = {'starting month': month, 'model': model_name, 'accuracy': model_result['accuracy'], 'confusion': str(model_result['confusion']), 'prediction': str(model_result['prediction'])}\n",
    "            df_backtest_result = pd.concat([df_backtest_result, pd.DataFrame(row, index=[0])], ignore_index=True)\n",
    "\n",
    "\n",
    "# select the best model for each month\n",
    "df_backtest_result['accuracy'] = df_backtest_result['accuracy'].astype(float)\n",
    "best_models = df_backtest_result.groupby('starting month').apply(lambda x: x.loc[x['accuracy'].idxmax()]).reset_index(drop=True)\n",
    "best_models.to_csv(f'../data/results/backtest_best_models_f{n_features[\"all_chip\"]}_chip.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/news_filtered_merged.csv')\n",
    "contents = data['content'].tolist()\n",
    "contents_cleaned = word_segmentation(contents[:100], ws, pos, ner)\n",
    "print(contents_cleaned[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all_cleaned.csv\n",
    "data_all = pd.DataFrame()\n",
    "for name, data in datas.items():\n",
    "    # concatenate the data\n",
    "    data_all = pd.concat([data_all, data])\n",
    "\n",
    "data_all.to_csv('../data/all_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('../data/all_cleaned.csv')\n",
    "df_chip = pd.read_csv('../data/籌碼數據-2年_by_date_standardized.csv')\n",
    "df_chip['date'] = pd.to_datetime(df_chip['date'])\n",
    "df_all['date'] = pd.to_datetime(df_all['date'])\n",
    "\n",
    "# merge two dataframes on date\n",
    "df_all_chip = df_all.merge(df_chip, on='date', how='left')\n",
    "df_all_chip.head()\n",
    "\n",
    "# save the merged data\n",
    "df_all_chip.to_csv('../data/all_cleaned_chip.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_chip = pd.read_csv('../data/all_cleaned_chip.csv')\n",
    "# drop nan rows with cleaned content\n",
    "df_all_chip = df_all_chip.dropna(subset=['content_cleaned'])\n",
    "# fill remaining nan with 0\n",
    "df_all_chip.fillna(0, inplace=True)\n",
    "df_all_chip.to_csv('../data/all_cleaned_chip.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p2_100 = pd.read_csv('../data/results/prob2_sorted_f100.csv')\n",
    "df_p2_100['num_features'] = 100\n",
    "df_p2_200 = pd.read_csv('../data/results/prob2_sorted_f200.csv')\n",
    "df_p2_200['num_features'] = 200\n",
    "df_p2_300 = pd.read_csv('../data/results/prob2_sorted_f300.csv')\n",
    "df_p2_300['num_features'] = 300\n",
    "df_p2_400 = pd.read_csv('../data/results/prob2_sorted_f400.csv')\n",
    "df_p2_400['num_features'] = 400\n",
    "df_p2_500 = pd.read_csv('../data/results/prob2_sorted_f500.csv')\n",
    "df_p2_500['num_features'] = 500\n",
    "df_p2_600 = pd.read_csv('../data/results/prob2_sorted_f600.csv')\n",
    "df_p2_600['num_features'] = 600\n",
    "df_p2_700 = pd.read_csv('../data/results/prob2_sorted_f700.csv')\n",
    "df_p2_700['num_features'] = 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the dataframes\n",
    "df_p2 = pd.concat([df_p2_100, df_p2_200, df_p2_300, df_p2_400, df_p2_500, df_p2_600, df_p2_700], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort by accuracy\n",
    "df_p2.sort_values(by='accuracy', ascending=False, inplace=True)\n",
    "df_p2.to_csv('../data/results/prob2_sorted.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_predictions_by_date = pd.read_csv('../data/results/predictions_by_date_f500_chip.csv')\n",
    "# compare final_prediction with label_day3, get confusion matrix\n",
    "confusion = pd.crosstab(df_predictions_by_date['final_prediction'], df_predictions_by_date['label_day3'])\n",
    "print(confusion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the make_decision rate\n",
    "make_decision_rate = np.mean(df_predictions_by_date['make_decision'])\n",
    "print(make_decision_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the rows where decision = -1\n",
    "df_predictions_by_date = df_predictions_by_date[df_predictions_by_date['final_prediction'] != -1]\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = np.mean(df_predictions_by_date['correct'])\n",
    "print(accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the accuracy and make_decision rate for each month\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df_predictions_by_date = pd.read_csv('../data/results/predictions_by_date_f500_chip.csv')\n",
    "df_predictions_by_date['month'] = pd.to_datetime(df_predictions_by_date['month'])\n",
    "# we shouldn't consider the case where final_prediction = -1 when calculating the accuracy\n",
    "df_predictions_by_date_2 = df_predictions_by_date[df_predictions_by_date['final_prediction'] != -1]\n",
    "# group by month\n",
    "df_grouped = df_predictions_by_date_2.groupby('month').agg({'correct': 'mean'}).reset_index()\n",
    "df_grouped['make_decision'] = df_predictions_by_date.groupby('month').agg({'make_decision': 'mean'}).reset_index()['make_decision']\n",
    "# plot the accuracy and make_decision rate\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_grouped['month'], df_grouped['correct'], label='Accuracy')\n",
    "plt.plot(df_grouped['month'], df_grouped['make_decision'], label='Make Decision Rate')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Rate')\n",
    "plt.title('Accuracy and Make Decision Rate by Month')\n",
    "plt.legend()\n",
    "# make the text size bigger\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
